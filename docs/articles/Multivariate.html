<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Multivariate • qacOutliers</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Multivariate">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">qacOutliers</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/qacOutliers.html"><span class="fa fas fa-chalkboard-teacher"></span> Getting Started</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-vignettes" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fas fa-book"></span> Vignettes</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-vignettes">
<li><a class="dropdown-item" href="../articles/Multivariate.html">Multivariate Outliers</a></li>
    <li><a class="dropdown-item" href="../articles/Univariate.html">Univariate Outliers</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html"><span class="fa fa-file-code-o"></span> Documentation</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Multivariate</h1>
            
      

      <div class="d-none name"><code>Multivariate.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">qacOutliers</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning: replacing previous import 'dplyr::combine' by 'gridExtra::combine'</span></span>
<span><span class="co">#&gt; when loading 'qacOutliers'</span></span>
<span><span class="co">#&gt; Warning: replacing previous import 'dplyr::lag' by 'stats::lag' when loading</span></span>
<span><span class="co">#&gt; 'qacOutliers'</span></span>
<span><span class="co">#&gt; Warning: replacing previous import 'dplyr::filter' by 'stats::filter' when</span></span>
<span><span class="co">#&gt; loading 'qacOutliers'</span></span>
<span><span class="co">#&gt; Warning: replacing previous import 'dbscan::as.dendrogram' by</span></span>
<span><span class="co">#&gt; 'stats::as.dendrogram' when loading 'qacOutliers'</span></span></code></pre></div>
<div class="section level2">
<h2 id="what-are-multivariate-outliers-how-do-you-detect-them">What are multivariate outliers? How do you detect them?<a class="anchor" aria-label="anchor" href="#what-are-multivariate-outliers-how-do-you-detect-them"></a>
</h2>
<p>A multivariate outlier is an outlier that can only be detected by
looking at two variables in combination. The graph below shows examples
of multivariate outliers. The data for this graph is taken from the
<code>Salaries</code> dataset from the <code>carData</code> package.</p>
<p><img src="Multivariate_files/figure-html/unnamed-chunk-2-1.png" width="700"></p>
<p>All of the red dots are multivariate outliers. The point labelled 1
on the graph is a clear example of a multivariate outlier. This person
has had 22 years since their PhD, a normal value for that variable, and
makes <code>$62,884</code> dollars, which is also a normal value for
salary. However, when combining these two features, a person who has had
22 years since their PhD and makes only <code>$62,884</code> is making
much less than other professors within their experience range.</p>
<p>The outliers in this graph were detected using the LoF method, and
more detail about that method can be provided below. This package
specifically focuses on four different methods for finding multivariate
outliers: kNN, LoF, mahalanobis distance, and iForest.</p>
</div>
<div class="section level2">
<h2 id="knn">kNN<a class="anchor" aria-label="anchor" href="#knn"></a>
</h2>
<p>kNN calculates the distances between a data point and its k-nearest
neighbors and assigns an outlier score based on that distance. The
principle that guides kNN is that outliers lay far away from their
neighbours, so each of the distances is interpreted within that context.
Because some variables in the data may have much larger ranges that
others (ex. a variable has a range from 1-10 and another has a range of
-10000 to 10000), the data is standardized before calculating the
distances.</p>
<p>Here is an example of the distances for the first 5 rows in
<code>mtcarsOutliers</code>, a dataset included with this package.</p>
<pre><code><span><span class="co">#&gt;           [,1]     [,2]     [,3]     [,4]     [,5]</span></span>
<span><span class="co">#&gt; [1,] 1.5229770 2.102410 2.265502 2.651939 2.664224</span></span>
<span><span class="co">#&gt; [2,] 1.5031299 1.509144 1.522977 1.568453 1.608401</span></span>
<span><span class="co">#&gt; [3,] 1.2561178 1.503130 1.728826 1.817606 1.983652</span></span>
<span><span class="co">#&gt; [4,] 0.3490918 1.045627 1.163944 1.331333 1.351668</span></span>
<span><span class="co">#&gt; [5,] 4.7397611 5.019562 5.024754 5.026238 5.106755</span></span></code></pre>
<p>After each of these distances are calculated, the average for each
row is calculated. Here are the average scores for the 5 rows shown
above. This step is why it’s important to standardize the data before
finding the distances.</p>
<pre><code><span><span class="co">#&gt; [1] 2.241410 1.542421 1.657866 1.048333 4.983414</span></span></code></pre>
<p>In this function, the next step involves creating a threshold for
declaring a point an outlier. To calculate this threshold, the function
takes the average of each row (after that row’s average has been
calculated), and adds 2 times the standard deviation of each row to that
number. In this case, the threshold is the number below.</p>
<pre><code><span><span class="co">#&gt; [1] 5.657686</span></span></code></pre>
<p>Outliers are considered any points with a score above the calculated
threshold. In this case, the outliers are shown below.</p>
<pre><code><span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: kNN</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 11 19</span></span>
<span><span class="co">#&gt; Outlier Score: 547.8248 393.1049</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : k = 5</span></span></code></pre>
<div class="section level3">
<h3 id="customizing-the-k-parameter">Customizing the k parameter<a class="anchor" aria-label="anchor" href="#customizing-the-k-parameter"></a>
</h3>
<p>The value <code>k</code> tells the function how many points to
consider as neighbors when identifying distances between each of the
points. The default value, 5, finds the distance between each point the
5 points that are closest to that point. The choice of <code>k</code>
significantly impacts the results, and smaller values are generally more
sensitive to outliers. You can supply your own value of <code>k</code>,
which may change the results of the function.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/multiOutliers.html">multiOutliers</a></span><span class="op">(</span><span class="va">mtcarsOutliers</span>, method <span class="op">=</span> <span class="st">"kNN"</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: kNN</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 11 19</span></span>
<span><span class="co">#&gt; Outlier Score: 568.7642 441.0597</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : k = 10</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="example-output">Example Output<a class="anchor" aria-label="anchor" href="#example-output"></a>
</h3>
<p>When using the kNN method with the default <code>k=5</code>, the
function returns:</p>
<ul>
<li>Method: “kNN”, indicating the method used.</li>
<li>Data: The dataset name.</li>
<li>Variables: The numeric columns considered for outlier
detection.</li>
<li>Row: Indices of rows identified as outliers.</li>
<li>Score: Average kNN distance scores of detected outliers.</li>
<li>Message: A summary message indicating whether outliers were
detected.</li>
<li>k: The number of nearest neighbors considered.</li>
</ul>
<pre><code><span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: kNN</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 11 19</span></span>
<span><span class="co">#&gt; Outlier Score: 547.8248 393.1049</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : k = 5</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="notes-and-considerations">Notes and Considerations<a class="anchor" aria-label="anchor" href="#notes-and-considerations"></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>Numeric Data Only: The kNN method requires numeric variables.
Non-numeric columns are automatically excluded.</p></li>
<li><p>Robustness: kNN does not assume a specific distribution of data,
so it is robust to non-normality, making it a better tool to handle
non-normal data than other outlier detection methods.</p></li>
</ol>
<p>To learn more about kNN and how it’s used in multivariate outlier
detection, visit these resources: - <a href="https://www.geeksforgeeks.org/k-nearest-neighbours/#" class="external-link">GeeksforGeeks.com</a>
- <a href="https://dualitytech.com/blog/anomaly-detection-k-nearest-neighbors/" class="external-link">Dualitytech.com</a>
- <a href="https://www.youtube.com/watch?v=HVXime0nQeI" class="external-link">StatQuest</a></p>
</div>
<div class="section level3">
<h3 id="graphical-output">Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output"></a>
</h3>
<p>Here is a graphical representation of the outliers shown above.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#add plot function when it's done</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="local-outlier-factor-lof">Local outlier factor (LoF)<a class="anchor" aria-label="anchor" href="#local-outlier-factor-lof"></a>
</h2>
<p>LoF for a point is the average density around the k-nearest neighbors
of the point divided by the density around the point itself. If the LoF
score is above 1, it ts more likely to be anomalous, if it is below 1,
it is less likely to be anomalous.</p>
<p>-describe how this works within our function</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/multiOutliers.html">multiOutliers</a></span><span class="op">(</span><span class="va">mtcarsOutliers</span>, method<span class="op">=</span><span class="st">"LoF"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: LoF</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 5 7 9 10 11 12 20 31 32</span></span>
<span><span class="co">#&gt; Outlier Score: 2.28365 2.719583 2.560179 1.530304 1.563598 1.804971 1.567824 2.950466 1.574599</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : minPts = 10</span></span></code></pre></div>
<p>-how to customize results (change threshold and stuff) -how to
intrepret results</p>
</div>
<div class="section level2">
<h2 id="mahalanobis">Mahalanobis<a class="anchor" aria-label="anchor" href="#mahalanobis"></a>
</h2>
<p>The Mahalanobis distance measures the distance of a point from the
center of a multivariate distribution while accounting for the
correlation between variables. This method identifies outliers by
calculating how far each point is from the data’s multivariate mean,
considering the covariance matrix of the data. This approach is
particularly useful when variables are highly correlated or have
different scales.</p>
<p>Before using the Mahalanobis distance, the function automatically
selects numeric columns from the dataset. Non-numeric variables are
excluded, ensuring compatibility with the method. The distances are then
calculated using the <code>outliers_mahalanobis</code> function from the
<code>Routliers</code> package.</p>
<p>Here is an example of calculating Mahalanobis distances for the
<code>mtcarsOutliers</code> dataset included with this package:</p>
<pre><code><span><span class="co">#&gt;         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive </span></span>
<span><span class="co">#&gt;          7.217210          3.682371          5.829785          1.981340 </span></span>
<span><span class="co">#&gt; Hornet Sportabout </span></span>
<span><span class="co">#&gt;         25.022031</span></span></code></pre>
<p>The outliers are identified by the function and their indices are
returned:</p>
<pre><code><span><span class="co">#&gt; Hornet Sportabout        Duster 360          Merc 230          Merc 280 </span></span>
<span><span class="co">#&gt;                 5                 7                 9                10 </span></span>
<span><span class="co">#&gt;    Toyota Corolla     Maserati Bora        Volvo 142E </span></span>
<span><span class="co">#&gt;                20                31                32</span></span></code></pre>
<p>Outliers are identified by comparing the Mahalanobis distance of each
point to a threshold derived from the chi-squared distribution. Points
with distances greater than the critical value at a specified
significance level (<code>alpha</code>) are flagged as outliers. The
default <code>alpha</code> is 0.05, which corresponds to a 95%
confidence level. You can customize this value to adjust the sensitivity
of the detection.</p>
<p>Here is the threshold for the dataset using the default
<code>alpha = 0.05</code>:</p>
<pre><code><span><span class="co">#&gt; [1] 18.30704</span></span></code></pre>
<p>The outliers identified are shown below:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/multiOutliers.html">multiOutliers</a></span><span class="op">(</span><span class="va">mtcarsOutliers</span>, method <span class="op">=</span> <span class="st">"mahalanobis"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: mahalanobis</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 5 7 9 10 12 20 31 32</span></span>
<span><span class="co">#&gt; Outlier Score: 25.11784 29.58581 27.30969 19.22256 26.99406 21.46034 27.95683 25.39721</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : alpha = 0.1</span></span></code></pre></div>
<div class="section level3">
<h3 id="customizing-the-alpha-parameter">Customizing the alpha parameter<a class="anchor" aria-label="anchor" href="#customizing-the-alpha-parameter"></a>
</h3>
<p>The <code>alpha</code> parameter in outliers_mahalanobis determines
the significance level for outlier detection. Lower values (e.g.,
<code>alpha = 0.01</code>) result in stricter thresholds, identifying
fewer points as outliers. You can modify <code>alpha</code> as
follows:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/multiOutliers.html">multiOutliers</a></span><span class="op">(</span><span class="va">mtcarsOutliers</span>, method <span class="op">=</span> <span class="st">"mahalanobis"</span>, alpha <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: mahalanobis</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 5 7 9 12 31 32</span></span>
<span><span class="co">#&gt; Outlier Score: 25.11784 29.58581 27.30969 26.99406 27.95683 25.39721</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : alpha = 0.01</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="example-output-1">Example Output<a class="anchor" aria-label="anchor" href="#example-output-1"></a>
</h3>
<p>When using the Mahalanobis method with the default
<code>alpha = 0.05</code>, the function returns:</p>
<ul>
<li>Method: “mahalanobis”, indicating the method used.</li>
<li>Data: The dataset name.</li>
<li>Variables: The numeric columns considered.</li>
<li>Row: Indices of rows identified as outliers.</li>
<li>Score: Mahalanobis distance scores of detected outliers.</li>
<li>Message: A summary message indicating whether outliers were
detected.</li>
<li>Alpha: The significance level used.</li>
</ul>
<pre><code><span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: mahalanobis</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Row: 5 7 9 12 20 31 32</span></span>
<span><span class="co">#&gt; Outlier Score: 25.11784 29.58581 27.30969 26.99406 21.46034 27.95683 25.39721</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span>
<span><span class="co">#&gt; Option 1 : alpha = 0.05</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="notes-and-considerations-1">Notes and Considerations<a class="anchor" aria-label="anchor" href="#notes-and-considerations-1"></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>Numeric Data Only: The Mahalanobis method requires numeric
variables. Non-numeric columns are automatically excluded.</p></li>
<li><p>Multivariate Normality: This method assumes the data follows a
multivariate normal distribution. Deviations from normality or the
presence of extreme outliers may affect the results.</p></li>
</ol>
<p>To learn more about Mahalanobis distance and how it’s used in
multivariate outlier detection, visit these resources: * <a href="https://www.statisticshowto.com/mahalanobis-distance/" class="external-link">Statisticshowto.com</a>
* <a href="https://builtin.com/data-science/mahalanobis-distance" class="external-link">Builtin.com</a></p>
</div>
</div>
<div class="section level2">
<h2 id="iforest">iForest<a class="anchor" aria-label="anchor" href="#iforest"></a>
</h2>
<p>iForest stands for isolation forest. First, it randomly selects a
variable, then randomly selects a value of that variable. This will work
for both quantitaive and categorical; if the variable is quantitative,
it will randomly pick a number in the range of the variable, and if the
variable is categorical it will randomly pick a level. Then it will
split the data using the value randomly selected eariler.</p>
<p>The iForest method repeats the above steps until all points are
separately in their own node. Then, for each data point, it counts how
many splits were needed to isolate it.</p>
<p>Because the selection of variables and values is random, this process
will return different results each time. Therefore, isolation trees are
repeated many times and the results are averaged over all trials. More
isolated points will have lower average path lengths. They are more
isolated from the rest of the data’s distribution, therefore they are
called outliers.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/multiOutliers.html">multiOutliers</a></span><span class="op">(</span><span class="va">mtcarsOutliers</span>, method<span class="op">=</span><span class="st">"iForest"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method: iForest</span></span>
<span><span class="co">#&gt; Dataset: mtcarsOutliers</span></span>
<span><span class="co">#&gt; Variables: mpg cyl disp hp drat wt qsec vs am gear carb iso_score</span></span>
<span><span class="co">#&gt; Row: Merc 230 Maserati Bora Toyota Corolla Hornet Sportabout Volvo 142E</span></span>
<span><span class="co">#&gt; Outlier Score: 0.5912855 0.5673572 0.5386362 0.4858491 0.4728747</span></span>
<span><span class="co">#&gt; Message:  Outliers detected</span></span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Caleb Henning, Larissa Xu, Angelica Crown, Ernie Little, Braeden Falzarano, Ral Reyes, Tegh Singh.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
